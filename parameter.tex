
\subsubsection{Verfahren zur Wahl der Einbettungsparameter}
\label{chapparams}
In den Einbettungstheoremen im vorigen Abschnitt wurde implizit vorausgesetzt, da"s wir
unendliche, rauschfreie Zeitreihen zur Verf"ugung haben. Dies ist jedoch bei Daten aus
realen Experimenten niemals der Fall. F"ur verrauschte, endliche Zeitreihen ist eine
vern"unftige Wahl der Verz"ogerungszeit $\delay$, die in den Einbettungstheoremen (bis auf
wenige Ausnahmen) beliebig sein kann, wesentlich. Au"serdem ist die Dimension des
einzubettenden Attraktors unbekannt und somit auch die Anzahl der ben"otigten
Verz"ogerungskoordinaten. Die folgenden Abschnitte werden sich mit diesen Punkten
besch"aftigen.


\paragraph{Die Einbettungsdimension}

Nach  dem Einbettungstheorem von \autor(Sauer \etal)  ist f"ur Einbettungsdimensionen\footnote{Mit
  Einbettungsdimension ist hier und im folgenden die Dimension unseres
  Rekonstruktionsphasenraumes gemeint, gleichg"ultig ob es sich bei der Abbildung in
  diesen Raum um eine Einbettung handelt oder nicht.} $\embed$ gr"o"ser als $2\fracdim$,
wobei $\fracdim$ die Kapazit"at des Attraktors ist, sichergestellt, da"s die
Verz"ogerungskoordinatenabbildung eine Einbettung ist. Nun ist bei experimentellen
Zeitreihen die Kapazit"at nicht a priori bekannt. Wir m"u"sten die Zeitreihe erst
einbetten, um die Kapazit"at bestimmen zu k"onnen.

Es stellt sich au"serdem die Frage, ob nicht schon kleinere Einbettungsdimensionen
$\embed\lt 2\fracdim$ Einbettungen liefern. Wie wir am Beispiel des R"ossler-Attraktors
gesehen haben, reicht hier eine Einbettungsdimension von $\embed=3$, w"ahrend Sauers
Theorem wegen $D_0\simeq2,07$ eine Einbettungsdimension von $\embed=5$ fordert. Die
Bedingung $\embed>2\fracdim$ ist nur notwendig, um absolut sicher zu gehen, da"s die
Abbildung eine Einbettung ist.

Um zu einer gegebenen Zeitreihe die optimale Einbettungsdimension zu bestimmen, sind eine
ganze Reihe von Verfahren entwickelt worden. Ein paar von diesen wollen wir im folgenden
vorstellen.
\begin{myitemize}
\item \rem(Falsche n"achste Nachbarn:) Diese Methode beruht darauf, da"s unter
  Einbettungen, die Nachbarschaftsbeziehungen zwischen benachbarten Punkten nicht
  ver\-"an\-dert werden \cite{Kennel92}. Wenn zwei Orbitpunkte im Originalphasenraum benachbart
  sind, so sind sie es auch im Einbettungsraum. Wenn nun $\embed$ eine ausreichende
  Einbettungsdimension ist, gilt dies auch f"ur $\embed+1$. Zwei Punkte die im $\R^\embed$
  benachbart sind, sollten also auch im $\R^{\embed+1}$ benachbart sein. Sind sie es
  nicht, kann die Abbildung in den $\R^\embed$ keine Einbettung gewesen sein.
  
  Man bestimmt nun zu einer Einbettungsdimension $\embed$ zu jedem Punkt $\x$ die
  \begriff(n"achsten Nachbarn), die innerhalb einer $\eps$-Umgebung  von $\x$ liegen. Die
  Nachbarpunkte, die beim "Ubergang zur Einbettungsdimension $\embed+1$ aus der
  $\eps$-Umgebung von $\x$ \metapher(entweichen), werden als \begriff(falsche n"achste
  Nachbarn) bezeichnet. F"ur ausreichende Einbettungsdimensionen wird das Verh"altnis
  zwischen falschen und \metapher(echten) n"achsten Nachbarn sehr klein, und wir
  k"onnen $\embed$ als Einbettungsdimension annehmen.
  
\item \rem(Attraktorvolumen:) Bei diesem Verfahren wird als erstes ein Ma"s f"ur das
  \metapher(Volumen) des rekonstruierten Attraktors definiert. $\buzvol{k,\embed}$ ist das
  mittlere von je $\embed+1$ Attraktorpunkten aufgespannte Volumen zur Verz"ogerung $k$ .
  Das Volumen, das von den $\embed+1$ Attraktorpunkten $\folge(\x,0,\embed)$ aufgespannt
  wird, betr"agt $\abs{\det\left(\x_1-\x_0,\dots,\x_\embed-\x_0\right)}$.  F"ur
  ausreichende Einbettungsdimensionen $d$ ist der Ausdruck
  $\log\buzvol{k,\embed+1}-\log\buzvol{k,\embed}$ f"ur alle $k$ in guter N"aherung
  konstant. Falls $d$ jedoch keine ausreichende Einbettungsdimension ist, kommt es f"ur
  bestimmte $k$ zu "Uberschneidungen von Trajektorien und damit zu einem \naja(zu
  kleinem) mittleren Volumen. Falls $d+1$ nun
  ausreichend ist, treten diese "Uberschneidungen nicht mehr auf. Dies "au"sert sich in
  einem Peak in der Auftragung von $\log\buzvol{k,\embed+1}-\log\buzvol{k,\embed}$ "uber
  $k$. Treten diese Peaks f"ur ein bestimmtes $\embed$ nicht mehr auf, so ist $\embed$
  eine ausreichende Einbettungsdimension \cite{Buzug94,Buzug90a}.

\item \rem(Singular Value Decomposition:) Hierbei
  wird zuerst eine Einbettung in einen hochdimensionalen Rekonstruktionsraum $\R^\embed$
  vorgenommen \cite{Broomhead-king}. F"ur niedrigdimensionale Dynamiken wird sich diese
  jedoch auf einen Unterraum $\subspace$ der Dimension $\minembed$ beschr"anken. Dieser
  Unterraum wird nun ermittelt und der Attraktor hieraus in den $\R^\minembed$ projiziert.
  Das Verfahren wird ausf"uhrlicher in Abschnitt~\ref{chapsvd} diskutiert.
\end{myitemize}
Die Reihe der Verfahren lie"se sich beliebig fortsetzen. Ein gute "Ubersicht
findet sich bei \autor(T. Buzug) \cite{Buzug94}. 

Wir wollen nun eines der noch nicht
aufgef"uhrten Verfahren genauer betrachten, da es sich u.a.\  gut dazu eignet,
Schwierigkeiten und Probleme bei der Analyse experimenteller chaotischer Systeme zu
demonstrieren. Bei diesem von \autor(Packard \etal) \cite{Packard80} entwickelten Verfahren stellen wir
uns den Attraktor eingebettet in eine $\mandim$-dimensionale Mannigfaltigkeit $\M$ vor.
Diese Mannigfaltigkeit sei ihrerseits eingebettet in den euklidischen Vektorraum
$\R^\embed$.  Schnitte von $\M$ mit $(\embed-1)$-di\-men\-sio\-nalen Hyperfl"achen
erzeugen nun im allgemeinen $(\mandim-1)$-dimensionale Mannigfaltigkeiten\footnote{Dies
  ist nur dann nicht der Fall, wenn der Schnitt leer ist oder die Hyperfl"ache tangential
  zu der Mannigfaltigkeit liegt.  Das erste werden wir im folgenden durch die Wahl der
  Hyperfl"achen ausschlie"sen. Letzteres ist f"ur pr"avalente Mengen von
  Mannigfaltigkeiten und Hyperfl"achen nicht der Fall.}.  Wenn wir die
Mannigfaltigkeit nun mit $\mandim$ paarweise orthogonalen Hyperfl"achen schneiden, wird
die Schnittmenge auf einen Punkt reduziert. Dies offeriert eine M"oglichkeit, die Dimension
der Mannigfaltigkeit $\M$, in der der Attraktor $\attr\subset\M$ liegt, zu bestimmen.

Als Schnittfl"achen betrachten wir die zu den Koordinatenachsen orthogonalen Teil\-r"aume
des $\R^\embed$. Die Tatsache, da"s ein Punkt innerhalb der Schnittmenge von $\mandim'$
dieser Teil\-r"aume mit der Mannigfaltigkeit $\M$ liegt, bringt uns Kenntnis "uber
$\mandim'$ seiner Koordinaten. Genau $\embed-\mandim'$ der Koordinaten sind unbestimmt,
wobei von diesen allerdings nur $\mandim-\mandim'$ Koordinaten unabh"angig sind, da die
Mannigfaltigkeit ja $\mandim$-dimensional ist. Wenn wir also $\mandim$ Schnitte
betrachten, sind dadurch alle Koordinaten eines Punktes aus $\M$ festgelegt.

Diese Tatsache kann nun durch \begriff(bedingte Wahrscheinlichkeiten) ausgedr"uckt werden.
Wir legen $\mandim'$ Koordinaten $x^0_1,\dots,x^0_{\mandim'}$ fest. Die
Rekonstruktionspunkte, deren erste $\mandim'$ Koordinaten gleich den $x^0_i$ sind, bilden
die Schnittmenge des Attraktors mit den durch $\mathcal{S}_i=\left\{ \x \vert
  x_i=x^0_i\right\}$ gegebenen Hyperfl"achen. Die Wahrscheinlichkeit, da"s ein Punkt aus
dieser Schnittmenge als $(\mandim'+1)$.\  Komponente den Wert $x$ aufweist, bezeichnen wir
mit $\Prob_{\mandim'}(x)=\Prob(x\vert x_1=x^0_1,\dots,x_{\mandim'}=x^0_{\mandim'})$. Mit
den anf"anglichen "Uberlegungen, da"s die genauen Koordinaten erst durch $\mandim$
Schnitte festgelegt sind, l"a"st sich nun schlie"sen, da"s die Verteilung von
$\Prob_{\mandim'}(x)$ f"ur $\mandim'<\mandim$ ausgedehnt, f"ur $\mandim'=\mandim$ dagegen
singul"ar werden mu"s.

F"ur die Berechnung der Verteilungen m"ussen wir aufgrund der endlichen Datenmenge (und
auch der endlichen Genauigkeit) von der exakten Gleichheit der Koordinaten abgehen und
Wahrscheinlichkeitsverteilungen $\Prob(i\vert i^0_1,\dots,i^0_{\mandim'})$ betrachten.
Hierbei ist der Wertebereich von $x$ in Intervalle $I_i=[x_i,x_{i+1}[$
aufgeteilt\footnote{Da wir hier Verz"ogerungskoordinaten betrachten, ist der Wertebereich
  f"ur alle Koordinaten gleich, und es macht Sinn, die Intervalle f"ur alle Koordinaten
  gleich zu w"ahlen.}.\@ $\Prob(i\vert i^0_1,\dots,i^0_{\mandim'})$ ist die
Wahrscheinlichkeit, da"s die $(\mandim'+1).$ Koordinate im Intervall $I_i$ liegt, unter der
Bedingung, da"s die ersten $\mandim'$ Koordinaten in den Intervallen
$I_{i^0_1},\dots,I_{i^0_{\mandim'}}$ liegen.

\noafterpage{
  \epsfigdouble{packdim/packdim1}{packdim/packdim2}{ Im Text beschriebene
    Wahrscheinlichkeitsverteilungen f"ur $m'=1$ (oben) bzw.\ $m'=2$ (unten). 
    Die Verz"ogerungszeit betrug $\delay=32\sample$. }
  {pakdim}{-0.2cm}

  \epsfigdouble{packdim/packdimb1}{packdim/packdimb2}{
    Wahrscheinlichkeitsverteilung wie in \psref{pakdim} aber mit Verz"ogerungszeit $\delay=35\sample$.
    }{pakdimb}{-0.2cm} 
}


Das Ergebnis einer solchen Berechnung ist in \psref{pakdim} am Beispiel des
R"ossler-Attraktors dargestellt. Die Anzahl der Intervalle betr"agt $200$, die
Vergleichskoordinaten wurden zu $x^0_1=0.0$ und $x^0_2=5.0$ (entspricht $i^0_1=124,
i^0_2=196$) gew"ahlt. F"ur die Verz"ogerungszeit $\delay=32\sample$ 
sieht man deutlich, da"s die Verteilung f"ur $\mandim'=2$ sehr scharf wird (\psref{pakdim} unten). Dies steht im
Einklang mit der lokalen Struktur des R"ossler-Attraktors, n"amlich einer Cantor-Menge
zweidimensionaler Schichten. Die Tatsache, da"s $\Prob$ auch neben dem Maximalwert nicht
verschwindet, ist  bedingt durch die endliche Rechengenauigkeit und durch
die \emph{eben nicht} exakt zweidimensionale Struktur des R"ossler-Attraktors.

In \psref{pakdimb} sind die Wahrscheinlichkeitsverteilungen f"ur
$\delay=35\sample$ bei sonst gleichen Parameterwerten dargestellt. Man erkennt deutlich ein
Problem dieses Verfahrens. F"ur geringf"ugig\footnote{Da"s diese Abweichung in Bezug auf
  die Bestimmbarkeit optimaler Verz"ogerungszeiten wirklich \begriff(geringf"ugig) ist,
  wird sich in Abschnitt~\ref{chapdelay} zeigen.} andere Verz"ogerungszeiten wird die
Verteilung $\Prob(i\vert i_1,i_2)$ nicht mehr singul"ar, sondern zeigt mehrere Maxima. Wir
m"u"sten f"ur diese Verz"ogerungszeit schlie"sen, da"s der Attraktor mindestens
dreidimensional ist.





Diese Tatsache w"are nicht so interessant, wenn die abweichenden Resultate bei
unterschiedlichen Parameterwerten nicht ein f"ur viele Verfahren der Zeitreihenanalyse
auftretendes Ph"anomen w"are. Es stellen sich hier mehrere Fragen:
\begin{myitemize}
\item Inwiefern ist dieses Ergebnis nur ein Artefakt unserer speziellen Parameterwahl? In
  dem hier betrachteten Fall taucht f"ur die meisten $\delay$ eine breite Verteilung auf.
  Ist nun eher dem Ergebnis f"ur das spezielle $\delay$, welches $\mandim=2$ impliziert,
  oder den Ergebnissen f"ur andere Verz"ogerungszeiten, welche eher $\mandim>2$ nahelegen,
  zu trauen? In diesem Fall ist dies durch unsere Systemkenntnis nat"urlich leicht zu
  entscheiden, aber wie sieht es bei unbekannten Systemen aus ?
\item Damit aussagekr"aftige Verteilungen erzeugt werden k"onnen, mu"s in der
  betrachteten Schnittmenge eine \naja(ausreichende) Zahl von Rekonstruktionspunkten
  liegen. Diese Zahl nimmt jedoch mit $(\Delta x)^{\mandim'}$ ab. Wir m"ussen also
  entweder mit einer geringeren Genauigkeit $(\Delta x)$ arbeiten, was h"aufig nicht
  akzeptabel ist, oder die Datenmenge entsprechend erh"ohen. Dies ist jedoch bei
  experimentellen, insbesondere bei biologischen oder medizinischen, Systemen oft
  nicht m"oglich.
\end{myitemize}

Das erste Problem der Parameterwahl wird in abgewandelter Form noch "ofter auftreten. Das
zweite, die exponentielle Zunahme der erforderlichen Datenmenge, wird in Abschnitt
\ref{chapcorrdim} genauer behandelt.

\paragraph{Die Verz"ogerungszeit}
\label{chapdelay}

Nach dem Einbettungstheorem von \autor(Sauer \etal) ist die Wahl der Verz"ogerungszeit bis auf wenige, in der Regel
erf"ullte Ausnahmen beliebig. Bei endlichen vielen, eventuell verrauschten Daten ist die Wahl
einer \naja(guten) Verz"ogerungszeit jedoch entscheidend f"ur eine erfolgreiche
Phasenraumrekonstruktion. In \psref{rekzeit} sind drei verschiedene Rekonstruktionen des
R"ossler-Attraktors aus der Zeitreihe in \psref{rekroe} (unten) zu den Verz"ogerungszeiten
$\delay=3\sample$, $30\sample$ und $200\sample$ abgebildet.

\afterpage{
\epsfigtriplebot{zeit/rectslow}{zeit/rectsmed}{zeit/rectshigh} {Rekonstruktion des
  R"ossler-Attraktors zu verschiedenen Verz"ogerungszeiten $\delay=3\sample$ (oben),
  $\delay=30\sample$  (unten links) und $\delay=200\sample$ (unten rechts). 
  Die Rekonstruktionen wurden im $\R^3$ vorgenommen,
  und hieraus f"ur die Abbildung in den $\R^2$ projiziert.  
}{rekzeit}{-0.2cm}
}

F"ur kleine Verz"ogerungen $k=\delay/\sample$ werden die Koordinaten ann"ahernd gleich:
\eqn{x_i\simeq x_{i+k}\simeq x_{i+2k};} die Rekonstruktionspunkte konzentrieren sich auf
die Hauptdiagonale des $\R^3$ (siehe \psref{rekzeit} oben). F"ur einen
Dimensionsalgorithmus, der nur mit endlicher Genauigkeit arbeiten kann, w"are dies kaum zu
unterscheiden, von einer eindimensionalen Struktur. In \psref{rekzeit} (unten rechts) ist
die Verz"ogerungszeit dagegen sehr hoch gew"ahlt.  Der Attraktor scheint wesentlich
komplizierter als der des Originalsystems. Bei rauschfreien Daten mag das noch nicht ganz
so gravierend sein. Bei Vorhandensein von Rauschen werden die Koordinaten der
rekonstruierten Punkte jedoch mit steigendem $\delay$ zunehmend unkorreliert, Trajektorien
durchkreuzen sich, und der Attraktor spannt den ganzen Phasenraum auf. Aufgrund dieser
Probleme ben"otigen wir also Verfahren, um vern"unftige Verz"ogerungszeiten bestimmen zu
k"onnen.


Methoden zur Bestimmung der Verz"ogerungszeit gibt es sehr viele.  Man kann sie grob
einteilen in Verfahren, die die Fensterl"ange, d.h. den zeitlichen Abstand
$\delay_w=(\embed-1)\delay$ der ersten und der letzten Komponente der
Rekonstruktionsvektoren, und in Verfahren, die direkt die Verz"ogerungszeit bestimmen.  Zu
den ersteren z"ahlt beispielsweise der Vorschlag von \autor(Broomhead) und \autor(King)
\cite{Broomhead-king}, $\delay_w$ als das Inverse einer das Frequenzspektrum begrenzenden
Frequenz\footnote{Eine genaue Definition oder ein Verfahren zur Bestimmung dieser Frequenz
  wird allerdings nicht angegeben.}  (engl.: band-limiting frequency) $f_\tmax$ zu w"ahlen
oder die Methode von \autor(Hilborn) und \autor(Ding) \cite{Hilborn-ding}, die, um
"Uberfaltungen des Attraktors zu vermeiden, $\delay_w$ umgekehrt proportional zum
gr"o"sten Lyapunov-Exponenten des Systems w"ahlen\footnote{Wie der gr"o"ste
  Lyapunov-Exponent ohne vern"unftige vorherige Rekonstruktion bestimmt werden soll,
  bleibt jedoch im Unklaren.}. Die Verfahren, die hier besprochen werden sollen, sind von
der zweiten Art und berechnen die Verz"ogerungszeit direkt.

Bevor mit der Besprechung dieser Verfahren begonnen wird, kurz ein Kommentar zur G"ute der
jeweils bestimmten Verz"ogerungszeiten. In vielen Publikationen ist zu lesen, dieses oder
jenes Verfahren sei in irgendeiner Weise anderen Methoden "uberlegen. Zum Teil werden
sogar ``G"utefunktionen'' definiert, die die "Uberlegenheit der Methode quantitativ,
anhand bestimmter Merkmale der Rekonstruktion, belegen sollen. Nach meiner Erfahrung macht
so ein Vorgehen wenig Sinn. Die ``beste'' Verz"ogerungszeit ist oft davon abh"angig, was
man genau mit der Zeitreihe anstellen m"ochte, d.h. ob man z.B.  Dimensionen oder
Lyapunov-Exponenten berechnen m"ochte oder eventuell Poincar\'e-Plots erstellen will. Oft
liefern die Verfahren gute Ansatzpunkte f"ur die Wahl der Verz"ogerungszeit, die genaue
Festlegung ist dann jedoch oft eine Sache von \naja(Trial and Error).
  

\subparagraph{Nulldurchgang der Autokorrelationsfunktion.} Ausgangspunkt f"ur die
Entwicklung der Verz"ogerungskoordinatenabbildung war die Idee, da"s die Dynamik des
Systems durch beliebige \emph{unabh"angige} Koordinaten dargestellt werden kann. Es ist
daher sinnvoll, die Verz"ogerung so zu w"ahlen, da"s die Koordinaten m"oglichst
unabh"angig werden. In der Signaltheorie wird ein Abweichungsma"s f"ur zwei Signale $X$
und $Y$ "uber deren \begriff(Kreuzkorrelation) $C(X,Y)$ definiert:
\eqnl[crosskorr]{C(X,Y)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{+T} (x(t)-\bar
  x)(y(t)-\bar y) \mathrm{dt}.} 
 Hierbei sind $x(t)$ und $y(t)$ die Werte der Signale zur
Zeit $t$ und $\bar x$ und $\bar y$ ihre Mittelwerte\footnotetext{Die hier definierte
  Kreuzkorrelation hei"st in der Signaltheorie eigentlich \begriff(Kreuzkovarianz). Beide
  Gr"o"sen unterscheiden sich dadurch, da"s bei der Kreuzkorrelation die Subtraktion der
  Mittelwerte {\em nicht} vorgenommen wird \cite{Lueke92}. Das Resultat ist, da"s beide
  Funktionen sich um eine Konstante -- das Produkt der Mittelwerte der Signale --
  unterscheiden. Entsprechendes gilt f"ur die Autokorrelationsfunktion. Wir wollen hier
  jedoch der in der Zeitreihenanalyse "ublichen Bezeichnungsweise folgen. }.  Die
Kreuzkorrelation wird maximal, wenn die Signale proportional, und null, wenn sie
orthogonal zueinander sind. $C(X,Y)=0$ impliziert auch die lineare Unabh"angigkeit der
Signale\footnote{Dies ist leicht einzusehen, wenn man die Signale $X$ und $Y$ als Elemente des
  Vektorraumes quadratintegrabler Funktionen mit dem Skalarprodukt $\langle X,Y \rangle=C(X,Y)$ auffa"st.}.
Bei der Verz"ogerungskoordinatenabbildung ist das Signal $Y$ nun genau das um die Zeit
$\delay$ verschobene Signal $X$ d.h.\ $y(t)=x(t+\delay)$.  Die rechte Seite von
\eqnref{crosskorr} geht damit "uber in die Definition der \begriff(Autokorrelationsfunktion):

\eqnl[autokorr1]{\ac(X,\delay)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{+T} (x(t)-\bar
  x)(x(t+\delay)-\bar x) \mathrm{dt} .} 

F"ur endliche diskrete Zeitreihen $\{x_i\}_{i=1\dots N}$ der L"ange $N$ und Zeiten
$k\sample$ kann die Autokorrelationsfunktion gen"ahert werden durch
\eqnl[autokorr2]{\ac(k\sample)=\frac{1}{N-k}\sum_{i=1}^{N-k} (x_i-\bar x)(x_{i+k}-\bar x)} 
mit
\eqnl[acmean]{\bar x = \frac{1}{N}\sum_{i=1}^N x_i .}

F"ur Zeiten $\delay$, die kein ganzzahliges Vielfaches der Sampling Time $\sample$ sind,
wird die Autokorrelationsfunktion linear approximiert durch:
\eqnl[autokorr3]{\ac(\delay)= \ac(k\sample) + \frac{\delay-k\sample}{\sample} \big(  \ac((k+1)\sample)
  - \ac(k\sample) \big)   ,}
wobei $k$ so bestimmt wird, da"s $k\sample<\tau<(k+1)\sample$ gilt.  Die
aufeinanderfolgenden Koordinaten werden maximal unabh"angig, wenn die Verz"ogerungszeit
$\delay=\tilde\delay_\ac$ so gew"ahlt wird, da"s $\ac(\tilde\delay_\ac)=0$ gilt.  Die
Autokorrelationsfunktion hat in der Regel mehrere Nullstellen. Um eine "Uberfaltung des
Attraktors zu vermeiden, sollte die Verz"ogerungszeit jedoch m"oglichst klein sein.  Aus
diesem Grund w"ahlen wir den ersten Nulldurchgang der Autokorrelationsfunktion als
Verz"ogerungszeit $\delay=\tilde\delay_\ac$.  Falls $\tilde\delay_\ac$ kein ganzzahliges Vielfaches
der Sampling Time $\sample$ ist (was im allgemeinen  der Fall ist), mu"s stattdessen die Verz"ogerungszeit
$\delay_\ac=k_\ac\sample$ benutzt werden, die am n"achsten bei $\tilde\delay_\ac$ liegt.

Die Implementierung des entsprechenden Algorithmus nach \eqnref{autokorr2} und
\eqnref{acmean} ist direkt durch Bildung der jeweiligen Summen durchf"uhrbar. Die Laufzeit
betr"agt $\order{N^2}$. Dies l"a"st sich jedoch beschleunigen durch Anwendung des
\begriff(Wiener-Khintchine-Theorems). Demnach ist die Autokorrelationsfunktion eines
Signals $X$ die Fourier-Transformierte des Leistungsspektrums $P_X(\omega)$. Wir ben"otigen
also nur zwei Fourier-Transformationen, deren Laufzeit bei Verwendung der
Fast-Fourier-Transformation (FFT) jeweils nur $\order{N\log N}$ betr"agt.  Algorithmen zur
Berechnung 
der FFT finden sich in nahezu jeder Mathematikbibliothek (siehe z.B. \cite{Numerical-recipes}), so da"s
hier nicht n"aher darauf eingegangen wird. Beispiele f"ur die
Autokorrelationsfunktion und die zur jeweiligen Verz"ogerung $k_\ac$ rekonstruierten
Attraktoren sind in \psref{acfigsa} und in \psref{acfigsb}  dargestellt.  

\noafterpage{
  \epsfigdouble{autocorr/roeac}{autocorr/roerec2}{
    Autokorrelationsfunktion $\ac(\delay)$ (oben) und der zur Verz"ogerungszeit $\delay_\ac=1.29$
    rekonstruierte Attraktor (unten) f"ur das R"ossler-System. 
}{acfigsa}{-0.2cm} 
\epsfigdouble{autocorr/lorac}{autocorr/lorrec2} {
    Autokorrelationsfunktion $\ac(\delay)$ (oben) und der zur Verz"ogerungszeit $\delay_\ac=2.50$
    rekonstruierte Attraktor (unten) f"ur das Lorenz-System. 
}{acfigsb}{-0.2cm} 
}

F"ur den R"ossler-Attraktor gelingt die Rekonstruktion mit der so gew"ahlten
Verz"ogerungszeit ganz passabel (siehe \psref{acfigsa} unten). Der Attraktor wirkt zwar
leicht "uberfaltet, die Messung von Dimensionen o."a.\ ist hier aber trotzdem gut
m"oglich. Beim Lorenz-Attraktor ist die Verz"ogerungszeit erkennbar zu gro"s. Der
Attraktor ist stark "uberfaltet und spannt fast den gesamten Phasenraum auf (siehe
\psref{acfigsb} unten). Dimensionsalgorithmen werden hier schwerlich auf vern"unftige
Werte konvergieren. Die zu hoch bestimmte Verz"ogerungszeit liegt in der Struktur des
Lorenz-Attraktors begr"undet. Die Orbits \naja(kreisen) meist mehrere Uml"aufe um einen
der instabilen Fixpunkte des Systems. W"ahrend dieser Zeit sind die Koordinaten daher
stark korreliert, da in dem einen Fl"ugel $x$ konstant positiv und in dem anderen konstant
negativ ist. Die hier bestimmte Verz"ogerungszeit entspricht also ungef"ahr der mittleren
Aufenthaltszeit des Systems auf einem der Fl"ugel.

Aufgrund dieser Schw"achen gibt es Ans"atze, statt des Nulldurchgangs das erste lokale
Minimum oder den Abfall auf $1/e$-tel des Anfangswertes $A(0)$ der Autokorrelationsfunktion zu betrachten. Diese
Ans"atze bringen zwar zum Teil bessere Ergebnisse, sind jedoch theoretisch nicht zu
begr"unden. Wir wollen sie hier deshalb beiseite lassen und ein allgemeineres Verfahren
besprechen.

\subparagraph{Redundanzanalyse.}  Wie wir gesehen haben stellt die lineare
Un\-ab\-h"an\-gig\-keit zweier Koordinaten nicht das optimale Kriterium f"ur eine gute
Verz"ogerungszeit dar. Die Probleme resultieren haupts"achlich daraus, da"s wir es mit
\emph{nichtlinearen} Systemen zu tun haben. Wir suchen eine allgemeinere Unabh"angigkeit
der Koordinaten.


\comment{Sei $X$ eine beliebige Zufallsvariable und $\Prob_X(i)$ die Wahrscheinlichkeit
  bei einer Messung von $X$ einen Wert im Intervall $[x_i,x_{i+1}[$ zu erhalten. Dann
  betr"agt die mittlere Information einer Messung von $X$}

Um die \begriff(allgemeine) Unabh"angigkeit zweier Koordinaten\footnote{Hiermit ist
  gemeint, da"s zwischen den Werten der Koordinaten kein (wie auch immer gearteter)
  funktionaler Zusammenhang besteht.} zu untersuchen,
m"ussen\korrektur(naja) wir uns Begriffen der Informationstheorie, insbesondere dem
\begriff(Shannonschen Informationsma"s), zuwenden. Die Messung einer Observablen $X$ habe
$n$ verschiedene Ausg"ange $x_i$, die mit den Wahrscheinlichkeiten $\Prob_X(i)$
auftreten\footnote{In der Informationstheorie betrachtet man im allgemeinen  Experimente, 
  die eine abz"ahlbare Menge von Me"sergebnissen liefern. Die m"oglichen Me"sergebnisse
  werden in diesem Rahmen "ublicherweise als \begriff(Ausg"ange) der Messung bezeichnet.}.
Dann liefert eine Messung der Observablen $X$ im Mittel die
\begriff(Information)\footnote{In der Informationstheorie wird statt des nat"urlichen
  Logarithmus der Zweierlogarithmus benutzt. Die sich ergebende Einheit der Information
  ist $1\,\bit$. Wir benutzen hier, wie in der Physik "ublich, den nat"urlichen
  Logarithmus. Die Einheit in der hier die Information gemessen wird ist das sogenannte
  $\nat$ (von engl.: natural). Beide Ma"se sind linear "uber $1\,\nat =\log_2
  e\,\bit\,$miteinander verkn"upft.}
\eqn{H(X)=-\sum_i \Prob_X(i) \ln \Prob_X(i).} 
Die
mittlere Information ist maximal, wenn alle Ausg"ange der Messung gleich wahrscheinlich
sind $\Prob_X(1)=\dots=\Prob_X(n)$. Sie wird umso kleiner, je st"arker die
Wahrscheinlichkeiten auf wenige Ausg"ange konzentriert sind.

Werden am betrachteten System zwei Messungen $X$ und $Y$ durchgef"uhrt, so betr"agt die
mittlere Information der kombinierten Messung:
\eqn{H(X,Y)=-\sum_{i,j} \Prob_{XY}(i,j) \ln\Prob_{XY}(i,j),} 
wobei $\Prob_{XY}(i,j)$ die Verbundwahrscheinlichkeit ist, da"s bei der
Messung von $X$ der Wert $x_i$ und bei der Messung von $Y$ der Wert $y_j$ festgestellt
wird\footnote{Die Verbundwahrscheinlichkeit $\Prob_{XY}(i,j)$ ist zu unterscheiden von der
  bedingten Wahrscheinlichkeit $\Prob_{Y|X}(i,j)$, die angibt mit welcher
  Wahrscheinlichkeit an $Y$ der Wert $y_j$ gemessen wird, \emph{wenn} die Messung von $X$
  den Wert $x_i$ ergab. Beide Wahrscheinlichkeiten sind verkn"upft "uber
  $\Prob_{XY}(i,j)=\Prob_{Y|X}(i,j)\Prob_X(i)$.}.  Die Information,
die die zus"atzliche Messung von $Y$ liefert, ist daher im allgemeinen  kleiner als $H(Y)$,
n"amlich:
\eqn{H(Y|X)=H(X,Y)-H(X).} 
Hierbei bezeichnet $H(Y|X)$ die mittlere Information der
Messung von $Y$ bei Kenntnis des Me"sergebnisses von $X$. Diese zus"atzliche Information
wird genau dann gleich $H(Y)$, wenn $X$ und $Y$ unabh"angige Me"sgr"o"sen sind. In diesem
Fall gilt f"ur die Verbundwahrscheinlichkeiten $\Prob_{XY}(i,j)=\Prob_X(i)\Prob_Y(j)$ und
wir erhalten:
\eqna{ H(X,Y)&=&-\sum_{i,j}\Prob_X(i) \Prob_Y(j) \{\ln \Prob_X(i)+\ln \Prob_Y(j)\} \nonumber \\
  &=& -\sum_i \Prob_X(i) \ln \Prob_X(i) - \sum_j \Prob_Y(j) \ln \Prob_Y(j) \nonumber \\
  &=& H(X)+H(Y),}
 somit $H(Y|X)=H(Y)$. In dem Fall, da"s zwischen $X$ und $Y$ ein direkter
funktionaler Zusammenhang besteht, liefert die Messung von $Y$ gar keine zus"atzliche
Information: 
\eqn{H(Y|X)=0.}

Wir definieren nun die \begriff(Redundanz) der kombinierten Messung $X,Y$. Redundanz
bedeutet im allgemeinen die Menge an "uberfl"ussiger Information. In diesem Fall ist dies die
Information, die sowohl in der Messung von $X$ als auch in der von $Y$ enthalten ist:
\eqnl[genredundancy]{R(X,Y)=H(X)+H(Y)-H(X,Y).}
 Die Redundanz $R$ wird genau dann minimal,
wenn die Me"sgr"o"sen maximal unabh"angig voneinander sind. Die Redundanz wird auch
manchmal als \begriff(Transinformation) (engl.: mutual information) bezeichnet. Diese
Bezeichnung r"uhrt daher, da"s die Information $R(X,Y)$ von $X$ nach $Y$ \slang("ubergeht)
oder \slang(flie"st). Die Transinformation kann in diesem Kontext zur Beschreibung von
Informationsfl"ussen in ausgehnten System verwendet werden\cite{Pawelzik91}.  In h"oheren
Dimensionen ergeben sich jedoch Unterschiede zwischen der Redundanz und Transinformation
\cite{Prichard95}.



Die Observablen $X$ und $Y$ m"ussen nicht notwendig verschiedene Me"sgr"o"sen darstellen.
Sie k"onnen auch die zeitversetzte Messung \emph{einer} Gr"o"se bedeuten.  Dies bringt uns
auf das Verfahren zur Bestimmung der Verz"ogerungszeit.  Wir definieren die zweite
Observable $Y$ als den zu einer Zeit $t+\delay$ gemessenen Wert von $X$, w"ahrend $X$ zur
Zeit $t$ gemessen wird. Damit geht \eqnref{genredundancy} "uber in
\eqnal[redundancy2]{R(X,\delay)&=&2H(X)-H(X,X_\delay) \nonumber \\
  &=& -2 \sum_i \Prob_X(i) \ln \Prob_X(i) + \sum_{i,j} \Prob_{X_\delay}(i,j) \log
  \Prob_{X_\delay}(i,j) ,} 
wobei $\Prob_{X_\delay}(i,j)$ die Wahrscheinlichkeit ist, da"s
eine Messung an $X$ zu einer beliebigen Zeit $t$ den Wert $x_i$ und zur Zeit $t+\delay$
den Wert $x_j$ liefert\comment{Genauer haben wir es hier mit der bedingten
  Wahrscheinlichkeit $\Prob(x_j=x(t+\delay)|x_i=x(t))$ zu tun.}.  Da wir nach
Unabh"angigkeit der Verz"ogerungskoordinaten gefragt haben, m"ussen wir also nur den Wert
von $\delay$ bestimmen, f"ur den $R(X,\delay)$ minimal wird. Wie wir schon in den
Betrachtungen zur Autokorrelationsfunktion festgestellt haben, nimmt die Korrelation
zeitlich versetzter Messungen exponentiell ab. Die Redundanz $R(X,\delay)$ strebt also
gegen null f"ur $\delay\to\infty$, erreicht ihr absolutes Minimum also f"ur sehr gro"se $\delay$. Da
wir sowohl an kleinen Verz"ogerungszeiten als auch an minimaler Redundanz interessiert
sind, w"ahlen wir als Verz"ogerung das erste lokale Minimum von $R$.
\epsfigtriplebot{density/density1}{density/density2}{density/density3} {Die
  Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$ f"ur das erste lokale Minimum
  ($\delay=0.162$, oben), das erste lokale Maximum ($\delay=0.234$, unten links) und das
  zweite lokale Maximum ($\delay=0.585$, unten rechts) der Redundanz $R(X,\delay)$ f"ur
  das Lorenz-System.
}{reddensities}{-0.2cm}

Die Wahrscheinlichkeitsverteilungen $P_{X_\delay}(i,j)$ f"ur das Lorenz-System bei
verschiedenen Verz"ogerungszeiten $\delay$ zeigt \psref{reddensities}. Flache Verteilungen
weisen auf niedrige Werte der Redundanz hin, w"ahrend Verteilungen, die haupts"achlich in
kleinen Bereichen lokalisiert sind, hohe Werte der Redundanz anzeigen.  Entsprechend
erkennt man in \psref{reddensities} (unten links) f"ur das erste lokale Maximum der
Redundanz, da"s die Verteilung der Wahrscheinlichkeiten stark auf die R"ander konzentriert
ist.  In \psref{reddensities} (oben und unten rechts) f"ur das erste bzw.\ zweite lokale
Minimum der Redundanz ist die Verteilung dagegen relativ flach. Die Verteilung f"ur das
zweite lokale Minimum l"a"st jedoch kaum noch (die vom Lorenz-System bekannte) Struktur
erkennen.

Die Berechnung von $R(X,\delay)$ erfordert die Berechnung der Wahrscheinlichkeiten
$\Prob_X(i)$ und der Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$. Diese Berechnung
ist nicht ganz trivial. Da wir zum einen kontinuierliche Me"swerte und zum anderen eine
begrenzte Anzahl Me"spunkte haben, m"ussen wir den Me"sraum geeignet partitionieren und
das Wahrscheinlichkeitsma"s dieser Partitionen bestimmen\footnote{Bei diskreten Me"swerten
  (d.h.\ endlich vielen Ausg"angen des Experiments) w"are verst"andlicherweise keine
  Partitionierung notwendig. Bei unendlich vielen kontinuierlichen Me"swerten k"onnte man
  dagegen die entsprechende integrale Form von \eqnref{redundancy2} verwenden. Hierbei
  gehen die Summen in Integrale und die Wahrscheinlichkeiten in die entsprechenden Dichten
  "uber. Da wir jedoch nur endlich viele Me"swerte haben, ist diese Vorgehensweise hier
  nicht anwendbar.}  Als Partitionen w"ahlen wir Intervalle $I^1_i=[\xi_i,\xi_{i+1}[$
bzw.\ Produkte von Intervallen $I^2_{i,j}=[\xi_i,\xi_{i+1}[\times[\xi_j,\xi_{j+1}[$. Die
Wahrscheinlichkeit $\Prob_X(i)$ ist dann gleich dem Anteil der Punkte $N_i$, der in das
Intervall $I^1_i$ f"allt, d.h.\ $\Prob_X(i)=N_i/N$. F"ur die Verbundwahrscheinlichkeiten
gilt entsprechend $\Prob_{x,\delay}(i,j)=N_{ij}/N$.

Offen ist noch wie die Grenzen der Intervalle $I^1_i$ gew"ahlt werden sollen. Es
existieren dazu verschiedene Ans"atze.
\begin{itemize}
\item Der erste benutzt einfach "aquidistante Grenzen $x_i$. Die L"ange der Intervalle
  wird auf einen bestimmten Teil der Varianz\footnote{Die Spannweite der Me"swerte w"are
    hier ein schlechtes Ma"s, da bei experimentellen Systemen "ofters \slang(Ausrei"ser)
    vorkommen, die die Spannweite stark verbreitern, sonst aber nicht viel beitragen.} der
  Me"swerte festgelegt. In den hier benutzten Beispielen wird die Intervall"ange auf
  $1/50$ der Varianz der Daten eingestellt.
\item Eine weitere Methode arbeitet mit Intervallen $I^1_i$, die so festgelegt werden,
  da"s die Wahrscheinlichkeiten $\Prob_X(i)$ f"ur alle Intervalle nahezu gleich sind.
  Hierzu wird die Zeitreihe $x_i$ der Gr"o"se nach sortiert. Die sortierte Reihe werde mit
  $x_{(i)}$ bezeichnet.  Offensichtlich haben Intervalle $[x_{(i)},x_{(i+k)}[$ die
  Wahrscheinlichkeit $\Prob_X(x_{(i)}\leq x<x_{(i+k)})=k/N$. Da die Wahrscheinlichkeiten
  f"ur alle Intervalle gleich sein sollen, legen wir f"ur die Intervallgrenzen
  $\xi_i=x_{(\lfloor\frac{N}{n}(i-1)\rfloor+1)}$ fest, wobei $n$ die Anzahl der Intervalle
  ist \footnote{$\lfloor x \rfloor$ ist die gr"o"ste ganze Zahl kleiner als $x$.}.
\item Ein von \autor(Fraser) und \autor(Swinney) vorgeschlagenes Verfahren bestimmt die
  Intervalle so, da"s die Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$ f"ur alle
  $(i,j)$ ann"ahernd gleich werden \cite{Fraser-swinney}. Gestartet wird mit einer
  Partitionierung, die aus einem einzigen Element
  $[\xi^0_1,\xi^0_2[\times[\xi^0_1,\xi^0_2[$ besteht. Dieses Element wird nun so in vier
  rechteckige Elemente zerlegt, da"s in jedem Element gleich viele Paare $(x_i,x_j)$ zu
  liegen kommen. Mit den durch diese Zerlegung entstandenen Elementen $\{
  [\xi^1_k,\xi^1_{k+1} [ \times [\xi^1_l, \xi^1_{l+1} [ \, \vert 1 \leq k , l \leq 2\}$
  wird nun in derselben Weise weiter verfahren. Nach $m$ Schritten erh"alt man so eine
  Partitionierung des Phasenraumes mit $4^m$ gleich wahrscheinlichen Elementen
  $\{[\xi^m_k,\xi^m_{k+1}[\times[\xi^m_l,\xi^m_{l+1}[\,\vert 1\leq k,l \leq 2^m\}$.
  Das Verfahren ist jedoch recht aufwendig und bringt meiner Einsch"atzung nach keine
  wesentlich besseren Ergebnisse.
\item \autor(K. Pawelzik) entwickelte ein Verfahren, um die Redundanz aus
  verallgemeinerten Korrelationsintegralen zu bestimmen \cite{Pawelzik91}.
  Es gilt $R(X,\delay,\eps)=2C_1(1,0,\eps)-C_1(2,\delay,\eps)$, wobei $C_1(d,\delay,\eps)$
  das verallgemeinerte Korrelationsintegral\footnote{Die verallgemeinerten
    Korrelationsintegrale $C_q(\eps)$ dienen (normalerweise) der Bestimmung der verallgemeinerten Dimensionen
    $D_q$, wobei $D_q = \lim_{\eps\to0}\log C_q(\eps)/\log\eps$ gilt. } zur Einbettungsdimension $d$ und 
  Verz"ogerungszeit $\delay$ ist \cite{Pawelzik-schuster}. $\eps$ entspricht der
  L"ange der Intervalle $I_i^1$ bei "aquidistanten Grenzen. 
\end{itemize}


Die Ergebnisse f"ur die Messung der Verz"ogerungszeit $\delay_R$ "uber das erste lokale
Minimum der Redundanz zeigt \psref{redresulta} und \psref{redresultb}. W"ahrend sich das Ergebnis beim
R"ossler-Attraktor nicht wesentlich ge"andert hat, ist das Resultat beim Lorenz-Attraktor
deutlich besser als bei der Rekonstruktion mit $\delay_\ac$. Die Verz"ogerungszeiten sind
bei beiden Systemen kleiner geworden: beim R"ossler-System von $1.29$ auf $1.16$, beim
Lorenz-System sogar von $2.50$ auf $0.162$.  

Die "uber die
Redundanzanalyse bestimmten Verz"ogerungszeiten $\delay_R$ sind immer kleiner oder gleich den
"uber den Nulldurchgang der Autokorrelationsfunktion bestimmten $\delay_\ac$. Man kann
sich dies in etwa so plausibel machen, da"s die bei der Verz"ogerungszeit $\delay_\ac$
vorliegende lineare Unabh"angigkeit der Koordinaten auch immer eine allgemeine
Unabh"angigkeit impliziert.

\noafterpage{
  \epsfigdouble{redundancy/roemut}{redundancy/roerec2}
  {Redundanz $R(\delay)$ (oben) und der zum ersten lokalen Minimum von $R(\delay)$
    rekonstruierten Attraktoren (unten) f"ur das R"ossler-System ($\delay_R=1.16$).
    }{redresulta}{-0.2cm} 
  \epsfigdouble{redundancy/lormut}{redundancy/lorrec2}
  {Redundanz $R(\delay)$ (oben) und der zum ersten lokalen Minimum von $R(\delay)$
    rekonstruierten Attraktoren (unten) f"ur das Lorenz-System ($\delay_R=0.162$).
    }{redresultb}{-0.2cm} 
} 

\comment{
\noafterpage{
  \epsfigdouble{autocorr/roecmp}{autocorr/lorcmp} {Vergleich der Autokorrelationsfunktion
    $\ac(\delay)$ mit der Redundanz $R(\delay)$. Die Verz"ogerungszeiten sind f"ur die
    Redundanz kleiner als die f"ur die Autokorrelationsfunktion.  }{acfigs3}{-0.2cm} }
}
\clearpage










