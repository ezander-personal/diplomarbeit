
\subsubsection{Verfahren zur Wahl der Einbettungsparameter}
\label{chapparams}
In den Einbettungstheoremen im vorigen Abschnitt wurde implizit vorausgesetzt, daß wir
unendliche, rauschfreie Zeitreihen zur Verfügung haben. Dies ist jedoch bei Daten aus
realen Experimenten niemals der Fall. Für verrauschte, endliche Zeitreihen ist eine
vernünftige Wahl der Verzögerungszeit $\delay$, die in den Einbettungstheoremen (bis auf
wenige Ausnahmen) beliebig sein kann, wesentlich. Außerdem ist die Dimension des
einzubettenden Attraktors unbekannt und somit auch die Anzahl der benötigten
Verzögerungskoordinaten. Die folgenden Abschnitte werden sich mit diesen Punkten
beschäftigen.


\paragraph{Die Einbettungsdimension}

Nach  dem Einbettungstheorem von \autor(Sauer \etal)  ist für Einbettungsdimensionen\footnote{Mit
  Einbettungsdimension ist hier und im folgenden die Dimension unseres
  Rekonstruktionsphasenraumes gemeint, gleichgültig ob es sich bei der Abbildung in
  diesen Raum um eine Einbettung handelt oder nicht.} $\embed$ größer als $2\fracdim$,
wobei $\fracdim$ die Kapazität des Attraktors ist, sichergestellt, daß die
Verzögerungskoordinatenabbildung eine Einbettung ist. Nun ist bei experimentellen
Zeitreihen die Kapazität nicht a priori bekannt. Wir müßten die Zeitreihe erst
einbetten, um die Kapazität bestimmen zu können.

Es stellt sich außerdem die Frage, ob nicht schon kleinere Einbettungsdimensionen
$\embed\lt 2\fracdim$ Einbettungen liefern. Wie wir am Beispiel des Rössler-Attraktors
gesehen haben, reicht hier eine Einbettungsdimension von $\embed=3$, während Sauers
Theorem wegen $D_0\simeq2,07$ eine Einbettungsdimension von $\embed=5$ fordert. Die
Bedingung $\embed>2\fracdim$ ist nur notwendig, um absolut sicher zu gehen, daß die
Abbildung eine Einbettung ist.

Um zu einer gegebenen Zeitreihe die optimale Einbettungsdimension zu bestimmen, sind eine
ganze Reihe von Verfahren entwickelt worden. Ein paar von diesen wollen wir im folgenden
vorstellen.
\begin{myitemize}
\item \rem(Falsche nächste Nachbarn:) Diese Methode beruht darauf, daß unter
  Einbettungen, die Nachbarschaftsbeziehungen zwischen benachbarten Punkten nicht
  ver\-än\-dert werden \cite{Kennel92}. Wenn zwei Orbitpunkte im Originalphasenraum benachbart
  sind, so sind sie es auch im Einbettungsraum. Wenn nun $\embed$ eine ausreichende
  Einbettungsdimension ist, gilt dies auch für $\embed+1$. Zwei Punkte die im $\R^\embed$
  benachbart sind, sollten also auch im $\R^{\embed+1}$ benachbart sein. Sind sie es
  nicht, kann die Abbildung in den $\R^\embed$ keine Einbettung gewesen sein.
  
  Man bestimmt nun zu einer Einbettungsdimension $\embed$ zu jedem Punkt $\x$ die
  \begriff(näch\-sten Nachbarn), die innerhalb einer $\eps$-Umgebung  von $\x$ liegen. Die
  Nachbarpunkte, die beim Übergang zur Einbettungsdimension $\embed+1$ aus der
  $\eps$-Umgebung von $\x$ \metapher(entweichen), werden als \begriff(falsche nächste
  Nachbarn) bezeichnet. Für ausreichende Einbettungsdimensionen wird das Verhältnis
  zwischen falschen und \metapher(echten) nächsten Nachbarn sehr klein, und wir
  können $\embed$ als Einbettungsdimension annehmen.
  
\item \rem(Attraktorvolumen:) Bei diesem Verfahren wird als erstes ein Maß für das
  \metapher(Volumen) des rekonstruierten Attraktors definiert. $\buzvol{k,\embed}$ ist das
  mittlere von je $\embed+1$ Attraktorpunkten aufgespannte Volumen zur Verzögerung $k$ .
  Das Volumen, das von den $\embed+1$ Attraktorpunkten $\folge(\x,0,\embed)$ aufgespannt
  wird, beträgt $\abs{\det\left(\x_1-\x_0,\dots,\x_\embed-\x_0\right)}$.  Für
  ausreichende Einbettungsdimensionen $d$ ist der Ausdruck
  $\log\buzvol{k,\embed+1}-\log\buzvol{k,\embed}$ für alle $k$ in guter Näherung
  konstant. Falls $d$ jedoch keine ausreichende Einbettungsdimension ist, kommt es für
  bestimmte $k$ zu Überschneidungen von Trajektorien und damit zu einem \naja(zu
  kleinem) mittleren Volumen. Falls $d+1$ nun
  ausreichend ist, treten diese Überschneidungen nicht mehr auf. Dies äußert sich in
  einem Peak in der Auftragung von $\log\buzvol{k,\embed+1}-\log\buzvol{k,\embed}$ über
  $k$. Treten diese Peaks für ein bestimmtes $\embed$ nicht mehr auf, so ist $\embed$
  eine ausreichende Einbettungsdimension \cite{Buzug94,Buzug90a}.

\item \rem(Singular Value Decomposition:) Hierbei
  wird zuerst eine Einbettung in einen hochdimensionalen Rekonstruktionsraum $\R^\embed$
  vorgenommen \cite{Broomhead-king}. Für niedrigdimensionale Dynamiken wird sich diese
  jedoch auf einen Unterraum $\subspace$ der Dimension $\minembed$ beschränken. Dieser
  Unterraum wird nun ermittelt und der Attraktor hieraus in den $\R^\minembed$ projiziert.
  Das Verfahren wird ausführlicher in Abschnitt~\ref{chapsvd} diskutiert.
\end{myitemize}
Die Reihe der Verfahren ließe sich beliebig fortsetzen. Ein gute Übersicht
findet sich bei \autor(Buzug) \cite{Buzug94}. 

Wir wollen nun eines der noch nicht
aufgeführten Verfahren genauer betrachten, da es sich u.a.\  gut dazu eignet,
Schwierigkeiten und Probleme bei der Analyse experimenteller chaotischer Systeme zu
demonstrieren. Bei diesem von \autor(Packard \etal) \cite{Packard80} entwickelten Verfahren stellen wir
uns den Attraktor eingebettet in eine $\mandim$-dimensionale Mannigfaltigkeit $\M$ vor.
Diese Mannigfaltigkeit sei ihrerseits eingebettet in den euklidischen Vektorraum
$\R^\embed$.  Schnitte von $\M$ mit $(\embed-1)$-di\-men\-sio\-nalen Hyperflächen
erzeugen nun im allgemeinen $(\mandim-1)$-dimensionale Mannigfaltigkeiten\footnote{Dies
  ist nur dann nicht der Fall, wenn der Schnitt leer ist oder die Hyperfläche tangential
  zu der Mannigfaltigkeit liegt.  Das erste werden wir im folgenden durch die Wahl der
  Hyperflächen ausschließen. Letzteres ist für prävalente Mengen von
  Mannigfaltigkeiten und Hyperflächen nicht der Fall.}.  Wenn wir die
Mannigfaltigkeit nun mit $\mandim$ paarweise orthogonalen Hyperflächen schneiden, wird
die Schnittmenge auf einen Punkt reduziert. Dies offeriert eine Möglichkeit, die Dimension
der Mannigfaltigkeit $\M$, in der der Attraktor $\attr\subset\M$ liegt, zu bestimmen.

Als Schnittflächen betrachten wir die zu den Koordinatenachsen orthogonalen Teil\-räume
des $\R^\embed$. Die Tatsache, daß ein Punkt innerhalb der Schnittmenge von $\mandim'$
dieser Teil\-räume mit der Mannigfaltigkeit $\M$ liegt, bringt uns Kenntnis über
$\mandim'$ seiner Koordinaten. Genau $\embed-\mandim'$ der Koordinaten sind unbestimmt,
wobei von diesen allerdings nur $\mandim-\mandim'$ Koordinaten unabhängig sind, da die
Mannigfaltigkeit ja $\mandim$-dimensional ist. Wenn wir also $\mandim$ Schnitte
betrachten, sind dadurch alle Koordinaten eines Punktes aus $\M$ festgelegt.

Diese Tatsache kann nun durch \begriff(bedingte Wahrscheinlichkeiten) ausgedrückt werden.
Wir legen $\mandim'$ Koordinaten $x^0_1,\dots,x^0_{\mandim'}$ fest. Die
Rekonstruktionspunkte, deren erste $\mandim'$ Koordinaten gleich den $x^0_i$ sind, bilden
die Schnittmenge des Attraktors mit den durch $\mathcal{S}_i=\left\{ \x \vert
  x_i=x^0_i\right\}$ gegebenen Hyperflächen. Die Wahrscheinlichkeit, daß ein Punkt aus
dieser Schnittmenge als $(\mandim'+1)$.\  Komponente den Wert $x$ aufweist, bezeichnen wir
mit $\Prob_{\mandim'}(x)=\Prob(x\vert x_1=x^0_1,\dots,x_{\mandim'}=x^0_{\mandim'})$. Mit
den anfänglichen Überlegungen, daß die genauen Koordinaten erst durch $\mandim$
Schnitte festgelegt sind, läßt sich nun schließen, daß die Verteilung von
$\Prob_{\mandim'}(x)$ für $\mandim'<\mandim$ ausgedehnt, für $\mandim'=\mandim$ dagegen
singulär werden muß.

Für die Berechnung der Verteilungen müssen wir aufgrund der endlichen Datenmenge (und
auch der endlichen Genauigkeit) von der exakten Gleichheit der Koordinaten abgehen und
Wahrscheinlichkeitsverteilungen $\Prob(i\vert i^0_1,\dots,i^0_{\mandim'})$ betrachten.
Hierbei ist der Wertebereich von $x$ in Intervalle $I_i=[x_i,x_{i+1}[$
aufgeteilt\footnote{Da wir hier Verzögerungskoordinaten betrachten, ist der Wertebereich
  für alle Koordinaten gleich, und es macht Sinn, die Intervalle für alle Koordinaten
  gleich zu wählen.}.\@ $\Prob(i\vert i^0_1,\dots,i^0_{\mandim'})$ ist die
Wahrscheinlichkeit, daß die $(\mandim'+1).$ Koordinate im Intervall $I_i$ liegt, unter der
Bedingung, daß die ersten $\mandim'$ Koordinaten in den Intervallen
$I_{i^0_1},\dots,I_{i^0_{\mandim'}}$ liegen.

\noafterpage{
  \epsfigdouble{packdim/packdim1}{packdim/packdim2}{ Im Text beschriebene
    Wahrscheinlichkeitsverteilungen für $m'=1$ (oben) bzw.\ $m'=2$ (unten). 
    Die Verzögerungszeit betrug $\delay=32\sample$. }
  {pakdim}{-0.2cm}

  \epsfigdouble{packdim/packdimb1}{packdim/packdimb2}{
    Wahrscheinlichkeitsverteilung wie in \psref{pakdim} aber mit Verzögerungszeit $\delay=35\sample$.
    }{pakdimb}{-0.2cm} 
}


Das Ergebnis einer solchen Berechnung ist in \psref{pakdim} am Beispiel des
Rössler-Attraktors dargestellt. Die Anzahl der Intervalle beträgt $200$, die
Vergleichskoordinaten wurden zu $x^0_1=0.0$ und $x^0_2=5.0$ (entspricht $i^0_1=124,
i^0_2=196$) gewählt. Für die Verzögerungszeit $\delay=32\sample$ 
sieht man deutlich, daß die Verteilung für $\mandim'=2$ sehr scharf wird (\psref{pakdim}
unten) und somit $\mandim=2$ gilt. Dies steht im
Einklang mit der lokalen Struktur des Rössler-Attraktors, nämlich einer Cantor-Menge
zweidimensionaler Schichten. Die Tatsache, daß $\Prob$ auch neben dem Maximalwert nicht
verschwindet, ist  bedingt durch die endliche Rechengenauigkeit und durch
die \emph{eben nicht} exakt zweidimensionale Struktur des Rössler-Attraktors.

In \psref{pakdimb} sind die Wahrscheinlichkeitsverteilungen für
$\delay=35\sample$ bei sonst gleichen Parameterwerten dargestellt. Die Verteilungen zeigen 
weder für $\mandim'=1$ noch für $\mandim'=2$ ein einzelnes scharfes Maximum. Man erkennt
hieran deutlich ein
Problem dieses Verfahrens. Für geringfügig andere Verzögerungszeiten\footnote{Daß diese Abweichung in Bezug auf
  die Bestimmbarkeit optimaler Verzögerungszeiten wirklich \begriff(geringfügig) ist,
  wird sich in Abschnitt~\ref{chapdelay} zeigen.} wird die
Verteilung $\Prob(i\vert i_1,i_2)$ nicht mehr singulär, sondern zeigt mehrere Maxima. Wir
müßten für diese Verzögerungszeit schließen, daß der Attraktor mindestens
dreidimensional ist.

Diese Tatsache wäre nicht so interessant, wenn die abweichenden Resultate bei
unterschiedlichen Parameterwerten nicht ein für viele Verfahren der Zeitreihenanalyse
auftretendes Phänomen wäre. Es stellen sich hier mehrere Fragen:
\begin{myitemize}
\item Inwiefern ist dieses Ergebnis nur ein Artefakt unserer speziellen Parameterwahl? In
  dem hier betrachteten Fall taucht für die meisten $\delay$ eine breite Verteilung auf.
  Ist nun eher dem Ergebnis für das spezielle $\delay$, welches $\mandim=2$ impliziert,
  oder den Ergebnissen für andere Verzögerungszeiten, welche eher $\mandim>2$ nahelegen,
  zu trauen? In diesem Fall ist dies durch unsere Systemkenntnis natürlich leicht zu
  entscheiden, aber wie sieht es bei unbekannten Systemen aus ?
\item Damit aussagekräftige Verteilungen erzeugt werden können, muß in der
  betrachteten Schnittmenge eine \naja(ausreichende) Zahl von Rekonstruktionspunkten
  liegen. Diese Zahl nimmt jedoch mit $(\Delta x)^{\mandim'}$ ab. Wir müssen also
  entweder mit einer geringeren Genauigkeit $(\Delta x)$ arbeiten, was häufig nicht
  akzeptabel ist, oder die Datenmenge entsprechend erhöhen. Dies ist jedoch bei
  experimentellen, insbesondere bei biologischen oder medizinischen, Systemen oft
  nicht möglich.
\end{myitemize}

Das erste Problem der Parameterwahl wird in abgewandelter Form noch öfter auftreten. Das
zweite, die exponentielle Zunahme der erforderlichen Datenmenge, wird in Abschnitt
\ref{chapcorrdim} genauer behandelt.

\paragraph{Die Verzögerungszeit}
\label{chapdelay}

Nach dem Einbettungstheorem von \autor(Sauer \etal) ist die Wahl der Verzögerungszeit bis auf wenige, in der Regel
erfüllte Ausnahmen beliebig. Bei endlichen vielen, eventuell verrauschten Daten ist die Wahl
einer \naja(guten) Verzögerungszeit jedoch entscheidend für eine erfolgreiche
Phasenraumrekonstruktion. In \psref{rekzeit} sind drei verschiedene Rekonstruktionen des
Rössler-Attraktors aus der Zeitreihe in \psref{rekroe} (unten) zu den Verzögerungszeiten
$\delay=3\sample$, $30\sample$ und $200\sample$ abgebildet.

\afterpage{
\epsfigtriplebot{zeit/rectslow}{zeit/rectsmed}{zeit/rectshigh} {Rekonstruktion des
  Rössler-Attraktors zu verschiedenen Verzögerungszeiten $\delay=3\sample$ (oben),
  $\delay=30\sample$  (unten links) und $\delay=200\sample$ (unten rechts). 
  Die Rekonstruktionen wurden im $\R^3$ vorgenommen,
  und hieraus für die Abbildung in den $\R^2$ projiziert.  
}{rekzeit}{-0.2cm}
}

Für kleine Verzögerungen $k=\delay/\sample$ werden die Koordinaten annähernd gleich:
\eqn{x_i\simeq x_{i+k}\simeq x_{i+2k};} die Rekonstruktionspunkte konzentrieren sich auf
die Hauptdiagonale des $\R^3$ (siehe \psref{rekzeit} oben). Für einen
Dimensionsalgorithmus, der nur mit endlicher Genauigkeit arbeiten kann, wäre dies kaum zu
unterscheiden, von einer eindimensionalen Struktur. In \psref{rekzeit} (unten rechts) ist
die Verzögerungszeit dagegen sehr hoch gewählt.  Der Attraktor scheint wesentlich
komplizierter als der des Originalsystems. Bei rauschfreien Daten mag das noch nicht ganz
so gravierend sein. Bei Vorhandensein von Rauschen werden die Koordinaten der
rekonstruierten Punkte jedoch mit steigendem $\delay$ zunehmend unkorreliert, Trajektorien
durchkreuzen sich, und der Attraktor spannt den ganzen Phasenraum auf. Aufgrund dieser
Probleme benötigen wir also Verfahren, um vernünftige Verzögerungszeiten bestimmen zu
können.


Methoden zur Bestimmung der Verzögerungszeit gibt es sehr viele.  Man kann sie grob
einteilen in Verfahren, die die Fensterlänge, d.h. den zeitlichen Abstand
$\delay_w=(\embed-1)\delay$ der ersten und der letzten Komponente der
Rekonstruktionsvektoren, und in Verfahren, die direkt die Verzögerungszeit bestimmen.  Zu
den ersteren zählt beispielsweise der Vorschlag von \autor(Broomhead) und \autor(King)
\cite{Broomhead-king}, $\delay_w$ als das Inverse einer das Frequenzspektrum begrenzenden
Frequenz\footnote{Eine genaue Definition oder ein Verfahren zur Bestimmung dieser Frequenz
  wird allerdings nicht angegeben.}  (engl.: band-limiting frequency) $f_\tmax$ zu wählen
oder die Methode von \autor(Hilborn) und \autor(Ding) \cite{Hilborn-ding}, die, um
Überfaltungen des Attraktors zu vermeiden, $\delay_w$ umgekehrt proportional zum
größten Lyapunov-Exponenten des Systems wählen\footnote{Wie der größte
  Lyapunov-Exponent ohne vernünftige vorherige Rekonstruktion bestimmt werden soll,
  bleibt jedoch im Unklaren.}. Die Verfahren, die hier besprochen werden sollen, sind von
der zweiten Art und berechnen die Verzögerungszeit direkt.

Bevor mit der Besprechung dieser Verfahren begonnen wird, kurz ein Kommentar zur Güte der
jeweils bestimmten Verzögerungszeiten. In vielen Publikationen ist zu lesen, dieses oder
jenes Verfahren sei in irgendeiner Weise anderen Methoden überlegen. Zum Teil werden
sogar ``Gütefunktionen'' definiert, die die Überlegenheit der Methode quantitativ,
anhand bestimmter Merkmale der Rekonstruktion, belegen sollen. Nach meiner Erfahrung macht
so ein Vorgehen wenig Sinn. Die ``beste'' Verzögerungszeit ist oft davon abhängig, was
man genau mit der Zeitreihe anstellen möchte, d.h. ob man z.B.  Dimensionen oder
Lyapunov-Exponenten berechnen möchte oder eventuell Poincar\'e-Plots erstellen will. Oft
liefern die Verfahren gute Ansatzpunkte für die Wahl der Verzögerungszeit, die genaue
Festlegung ist dann jedoch oft eine Sache von \naja(Trial and Error).
  

\subparagraph{Nulldurchgang der Autokorrelationsfunktion.} Ausgangspunkt für die
Entwicklung der Verzögerungskoordinatenabbildung war die Idee, daß die Dynamik des
Systems durch beliebige \emph{unabhängige} Koordinaten dargestellt werden kann. Es ist
daher sinnvoll, die Verzögerung so zu wählen, daß die Koordinaten möglichst
unabhängig werden. In der Signaltheorie wird ein Abweichungsmaß für zwei Signale $X$
und $Y$ über deren \begriff(Kreuzkorrelation) $C(X,Y)$ definiert:
\eqnl[crosskorr]{C(X,Y)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{+T} (x(t)-\bar
  x)(y(t)-\bar y) \mathrm{dt}.} 
 Hierbei sind $x(t)$ und $y(t)$ die Werte der Signale zur
Zeit $t$ und $\bar x$ und $\bar y$ ihre Mittelwerte\footnote{Die hier definierte
  Kreuzkorrelation heißt in der Signaltheorie eigentlich \begriff(Kreuzkovarianz). Beide
  Größen unterscheiden sich dadurch, daß bei der Kreuzkorrelation die Subtraktion der
  Mittelwerte {\em nicht} vorgenommen wird \cite{Lueke92}. Das Resultat ist, daß beide
  Funktionen sich um eine Konstante -- das Produkt der Mittelwerte der Signale --
  unterscheiden. Entsprechendes gilt für die Autokorrelationsfunktion. Wir wollen hier
  jedoch der in der Zeitreihenanalyse üblichen Bezeichnungsweise folgen. }.  Die
Kreuzkorrelation wird maximal, wenn die Signale proportional, und null, wenn sie
orthogonal zueinander sind. $C(X,Y)=0$ impliziert auch die lineare Unabhängigkeit der
Signale\footnote{Dies ist leicht einzusehen, wenn man die Signale $X$ und $Y$ als Elemente des
  Vektorraumes quadratintegrabler Funktionen mit dem Skalarprodukt $\langle X,Y \rangle=C(X,Y)$ auffaßt.}.
Bei der Verzögerungskoordinatenabbildung ist das Signal $Y$ nun genau das um die Zeit
$\delay$ verschobene Signal $X$ d.h.\ $y(t)=x(t+\delay)$.  Die rechte Seite von
\eqnref{crosskorr} geht damit über in die Definition der \begriff(Autokorrelationsfunktion):

\eqnl[autokorr1]{\ac(X,\delay)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{+T} (x(t)-\bar
  x)(x(t+\delay)-\bar x) \mathrm{dt} .} 

Für endliche diskrete Zeitreihen $\{x_i\}_{i=1\dots N}$ der Länge $N$ und Zeiten
$k\sample$ kann die Autokorrelationsfunktion genähert werden durch
\eqnl[autokorr2]{\ac(k\sample)=\frac{1}{N-k}\sum_{i=1}^{N-k} (x_i-\bar x)(x_{i+k}-\bar x)} 
mit
\eqnl[acmean]{\bar x = \frac{1}{N}\sum_{i=1}^N x_i .}

Für Zeiten $\delay$, die kein ganzzahliges Vielfaches der Sampling Time $\sample$ sind,
wird die Autokorrelationsfunktion linear approximiert durch:
\eqnl[autokorr3]{\ac(\delay)= \ac(k\sample) + \frac{\delay-k\sample}{\sample} \big(  \ac((k+1)\sample)
  - \ac(k\sample) \big)   ,}
wobei $k$ so bestimmt wird, daß $k\sample<\tau<(k+1)\sample$ gilt.  Die
aufeinanderfolgenden Koordinaten werden maximal unabhängig, wenn die Verzögerungszeit
$\delay=\tilde\delay_\ac$ so gewählt wird, daß $\ac(\tilde\delay_\ac)=0$ gilt.  Die
Autokorrelationsfunktion hat in der Regel mehrere Nullstellen. Um eine Überfaltung des
Attraktors zu vermeiden, sollte die Verzögerungszeit jedoch möglichst klein sein.  Aus
diesem Grund wählen wir den ersten Nulldurchgang der Autokorrelationsfunktion als
Verzögerungszeit $\delay=\tilde\delay_\ac$.  Falls $\tilde\delay_\ac$ kein ganzzahliges Vielfaches
der Sampling Time $\sample$ ist (was im allgemeinen  der Fall ist), muß stattdessen die Verzögerungszeit
$\delay_\ac=k_\ac\sample$ benutzt werden, die am nächsten bei $\tilde\delay_\ac$ liegt.

Die Implementierung des entsprechenden Algorithmus nach \eqnref{autokorr2} und
\eqnref{acmean} ist direkt durch Bildung der jeweiligen Summen durchführbar. Die Laufzeit
beträgt $\order{N^2}$. Dies läßt sich jedoch beschleunigen durch Anwendung des
\begriff(Wiener-Khintchine-Theorems). Demnach ist die Autokorrelationsfunktion eines
Signals $X$ die Fourier-Transformierte des Leistungsspektrums $P_X(f)$. Wir benötigen
also nur zwei Fourier-Transformationen, deren Laufzeit bei Verwendung der
Fast-Fourier-Transformation (FFT) jeweils nur \linebreak[4]  $\mathcal{O}(N\log N)$  beträgt.  Algorithmen zur
Berechnung 
der FFT finden sich in nahezu jeder Mathematikbibliothek (siehe z.B. \cite{Numerical-recipes}), so daß
hier nicht näher darauf eingegangen wird. Beispiele für die
Autokorrelationsfunktion und die zur jeweiligen Verzögerung $k_\ac$ rekonstruierten
Attraktoren sind in \psref{acfigsa} und in \psref{acfigsb}  dargestellt.  

\noafterpage{
  \epsfigdouble{autocorr/roeac}{autocorr/roerec2}{
    Autokorrelationsfunktion $\ac(\delay)$ (oben) und der zur Verzögerungszeit $\delay_\ac=1.29$
    rekonstruierte Attraktor (unten) für das Rössler-System. 
}{acfigsa}{-0.2cm} 
\epsfigdouble{autocorr/lorac}{autocorr/lorrec2} {
    Autokorrelationsfunktion $\ac(\delay)$ (oben) und der zur Verzögerungszeit $\delay_\ac=2.50$
    rekonstruierte Attraktor (unten) für das Lorenz-System. 
}{acfigsb}{-0.2cm} 
}

Für den Rössler-Attraktor gelingt die Rekonstruktion mit der so gewählten
Verzögerungszeit ganz passabel (siehe \psref{acfigsa} unten). Der Attraktor wirkt zwar
leicht überfaltet, die Messung von Dimensionen o.ä.\ ist hier aber trotzdem gut
möglich. Beim Lorenz-Attraktor ist die Verzögerungszeit erkennbar zu groß. Der
Attraktor ist stark überfaltet und spannt fast den gesamten Phasenraum auf (siehe
\psref{acfigsb} unten). Dimensionsalgorithmen werden hier schwerlich auf vernünftige
Werte konvergieren. Die zu hoch bestimmte Verzögerungszeit liegt in der Struktur des
Lorenz-Attraktors begründet. Die Orbits \naja(kreisen) meist mehrere Umläufe um einen
der instabilen Fixpunkte des Systems. Während dieser Zeit sind die Koordinaten daher
stark korreliert, da in dem einen Flügel $x$ konstant positiv und in dem anderen konstant
negativ ist. Die hier bestimmte Verzögerungszeit entspricht also ungefähr der mittleren
Aufenthaltszeit des Systems auf einem der Flügel.

Aufgrund dieser Schwächen gibt es Ansätze, statt des Nulldurchgangs das erste lokale
Minimum oder den Abfall auf ein $e$-tel des Anfangswertes $A(0)$ der Autokorrelationsfunktion zu betrachten. Diese
Ansätze bringen zwar zum Teil bessere Ergebnisse, sind jedoch theoretisch nicht zu
begründen. Wir wollen sie hier deshalb beiseite lassen und ein allgemeineres Verfahren
besprechen.

\subparagraph{Redundanzanalyse.} \label{chapredundancy} Wie wir gesehen haben stellt die lineare
Un\-ab\-hän\-gig\-keit zweier Koordinaten nicht das optimale Kriterium für eine gute
Verzögerungszeit dar. Die Probleme resultieren hauptsächlich daraus, daß wir es mit
\emph{nichtlinearen} Systemen zu tun haben. Wir suchen eine allgemeinere Unabhängigkeit
der Koordinaten.


\comment{Sei $X$ eine beliebige Zufallsvariable und $\Prob_X(i)$ die Wahrscheinlichkeit
  bei einer Messung von $X$ einen Wert im Intervall $[x_i,x_{i+1}[$ zu erhalten. Dann
  beträgt die mittlere Information einer Messung von $X$}

Um die \begriff(allgemeine) Unabhängigkeit zweier Koordinaten\footnote{Hiermit ist
  gemeint, daß zwischen den Werten der Koordinaten kein (wie auch immer gearteter)
  funktionaler Zusammenhang besteht.} zu untersuchen,
müssen\korrektur(naja) wir uns Begriffen der Informationstheorie, insbesondere dem
\begriff(Shannonschen Informationsmaß), zuwenden. Die Messung einer Observablen $X$ habe
$n$ verschiedene Ausgänge $x_i$, die mit den Wahrscheinlichkeiten $\Prob_X(i)$
auftreten\footnote{In der Informationstheorie betrachtet man im allgemeinen  Experimente, 
  die eine abzählbare Menge von Meßergebnissen liefern. Die möglichen Meßergebnisse
  werden in diesem Rahmen üblicherweise als \begriff(Ausgänge) der Messung bezeichnet.}.
Dann liefert eine Messung der Observablen $X$ im Mittel die
\begriff(Information)\footnote{In der Informationstheorie wird statt des natürlichen
  Logarithmus der Zweierlogarithmus benutzt. Die sich ergebende Einheit der Information
  ist $1\,\bit$. Wir benutzen hier, wie in der Physik üblich, den natürlichen
  Logarithmus. Die Einheit in der hier die Information gemessen wird ist das sogenannte
  $\nat$ (von engl.: natural). Beide Maße sind linear über $1\,\nat =\log_2
  e\,\bit\,$miteinander verknüpft.}
\eqn{H(X)=-\sum_i \Prob_X(i) \ln \Prob_X(i).} 
Die
mittlere Information ist maximal, wenn alle Ausgänge der Messung gleich wahrscheinlich
sind $\Prob_X(1)=\dots=\Prob_X(n)$. Sie wird umso kleiner, je stärker die
Wahrscheinlichkeiten auf wenige Ausgänge konzentriert sind.

Werden am betrachteten System zwei Messungen $X$ und $Y$ durchgeführt, so beträgt die
mittlere Information der kombinierten Messung:
\eqn{H(X,Y)=-\sum_{i,j} \Prob_{XY}(i,j) \ln\Prob_{XY}(i,j),} 
wobei $\Prob_{XY}(i,j)$ die Verbundwahrscheinlichkeit ist, daß bei der
Messung von $X$ der Wert $x_i$ und bei der Messung von $Y$ der Wert $y_j$ festgestellt
wird\footnote{Die Verbundwahrscheinlichkeit $\Prob_{XY}(i,j)$ ist zu unterscheiden von der
  bedingten Wahrscheinlichkeit $\Prob_{Y|X}(i,j)$, die angibt mit welcher
  Wahrscheinlichkeit an $Y$ der Wert $y_j$ gemessen wird, \emph{wenn} die Messung von $X$
  den Wert $x_i$ ergab. Beide Wahrscheinlichkeiten sind verknüpft über
  $\Prob_{XY}(i,j)=\Prob_{Y|X}(i,j)\Prob_X(i)$.}.  Die Information,
die die zusätzliche Messung von $Y$ liefert, ist daher im allgemeinen  kleiner als $H(Y)$,
nämlich:
\eqn{H(Y|X)=H(X,Y)-H(X).} 
Hierbei bezeichnet $H(Y|X)$ die mittlere Information der
Messung von $Y$ bei Kenntnis des Meßergebnisses von $X$. Diese zusätzliche Information
wird genau dann gleich $H(Y)$, wenn $X$ und $Y$ unabhängige Meßgrößen sind. In diesem
Fall gilt für die Verbundwahrscheinlichkeiten $\Prob_{XY}(i,j)=\Prob_X(i)\Prob_Y(j)$ und
wir erhalten:
\eqna{ H(X,Y)&=&-\sum_{i,j}\Prob_X(i) \Prob_Y(j) \{\ln \Prob_X(i)+\ln \Prob_Y(j)\} \nonumber \\
  &=& -\sum_i \Prob_X(i) \ln \Prob_X(i) - \sum_j \Prob_Y(j) \ln \Prob_Y(j) \nonumber \\
  &=& H(X)+H(Y),}
 somit $H(Y|X)=H(Y)$. In dem Fall, daß zwischen $X$ und $Y$ ein direkter
funktionaler Zusammenhang besteht, liefert die Messung von $Y$ gar keine zusätzliche
Information: 
\eqn{H(Y|X)=0.}

Wir definieren nun die \begriff(Redundanz) der kombinierten Messung $X,Y$. Redundanz
bedeutet im allgemeinen die Menge an überflüssiger Information. In diesem Fall ist dies die
Information, die sowohl in der Messung von $X$ als auch in der von $Y$ enthalten ist:
\eqnl[genredundancy]{R(X,Y)=H(X)+H(Y)-H(X,Y).}
 Die Redundanz $R$ wird genau dann minimal,
wenn die Meßgrößen maximal unabhängig voneinander sind. Die Redundanz wird auch
manchmal als \begriff(Transinformation) (engl.: mutual information) bezeichnet. Diese
Bezeichnung rührt daher, daß die Information $R(X,Y)$ von $X$ nach $Y$ \slang(übergeht)
oder \slang(fließt). Die Transinformation kann in diesem Kontext zur Beschreibung von
Informationsflüssen in ausgehnten System verwendet werden\cite{Pawelzik91}.  In höheren
Dimensionen ergeben sich jedoch Unterschiede zwischen der Redundanz und Transinformation
\cite{Prichard95}.



Die Observablen $X$ und $Y$ müssen nicht notwendig verschiedene Meßgrößen darstellen.
Sie können auch die zeitversetzte Messung \emph{einer} Größe bedeuten.  Dies bringt uns
auf das Verfahren zur Bestimmung der Verzögerungszeit.  Wir definieren die zweite
Observable $Y$ als den zu einer Zeit $t+\delay$ gemessenen Wert von $X$, während $X$ zur
Zeit $t$ gemessen wird. Damit geht \eqnref{genredundancy} über in
\eqnal[redundancy2]{R(X,\delay)&=&2H(X)-H(X,X_\delay) \nonumber \\
  &=& -2 \sum_i \Prob_X(i) \ln \Prob_X(i) + \sum_{i,j} \Prob_{X_\delay}(i,j) \log
  \Prob_{X_\delay}(i,j) ,} 
wobei $\Prob_{X_\delay}(i,j)$ die Wahrscheinlichkeit ist, daß
eine Messung an $X$ zu einer beliebigen Zeit $t$ den Wert $x_i$ und zur Zeit $t+\delay$
den Wert $x_j$ liefert\comment{Genauer haben wir es hier mit der bedingten
  Wahrscheinlichkeit $\Prob(x_j=x(t+\delay)|x_i=x(t))$ zu tun.}.  Da wir nach
Unabhängigkeit der Verzögerungskoordinaten gefragt haben, müssen wir also nur den Wert
von $\delay$ bestimmen, für den $R(X,\delay)$ minimal wird. Wie wir schon in den
Betrachtungen zur Autokorrelationsfunktion festgestellt haben, nimmt die Korrelation
zeitlich versetzter Messungen exponentiell ab. Die Redundanz $R(X,\delay)$ strebt also
gegen null für $\delay\to\infty$, erreicht ihr absolutes Minimum also für sehr große $\delay$. Da
wir sowohl an kleinen Verzögerungszeiten als auch an minimaler Redundanz interessiert
sind, wählen wir als Verzögerung das erste lokale Minimum von $R$.
\epsfigtriplebot{density/density1}{density/density2}{density/density3} {Die
  Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$ für das erste lokale Minimum
  ($\delay=0.162$, oben), das erste lokale Maximum ($\delay=0.234$, unten links) und das
  zweite lokale Maximum ($\delay=0.585$, unten rechts) der Redundanz $R(X,\delay)$ für
  das Lorenz-System.
}{reddensities}{-0.2cm}

Die Wahrscheinlichkeitsverteilungen $P_{X_\delay}(i,j)$ für das Lorenz-System bei
verschiedenen Verzögerungszeiten $\delay$ zeigt \psref{reddensities}. Flache Verteilungen
weisen auf niedrige Werte der Redundanz hin, während Verteilungen, die hauptsächlich in
kleinen Bereichen lokalisiert sind, hohe Werte der Redundanz anzeigen.  Entsprechend
erkennt man in \psref{reddensities} (unten links) für das erste lokale Maximum der
Redundanz, daß die Verteilung der Wahrscheinlichkeiten stark auf die Ränder konzentriert
ist.  In \psref{reddensities} (oben und unten rechts) für das erste bzw.\ zweite lokale
Minimum der Redundanz ist die Verteilung dagegen relativ flach. Die Verteilung für das
zweite lokale Minimum läßt jedoch kaum noch (die vom Lorenz-System bekannte) Struktur
erkennen.

Die Berechnung von $R(X,\delay)$ erfordert die Berechnung der Wahrscheinlichkeiten
$\Prob_X(i)$ und der Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$. Diese Berechnung
ist nicht ganz trivial. Da wir zum einen kontinuierliche Meßwerte und zum anderen eine
begrenzte Anzahl Meßpunkte haben, müssen wir den Meßraum geeignet partitionieren und
das Wahrscheinlichkeitsmaß dieser Partitionen bestimmen\footnote{Bei diskreten Meßwerten
  (d.h.\ endlich vielen Ausgängen des Experiments) wäre verständlicherweise keine
  Partitionierung notwendig. Bei unendlich vielen kontinuierlichen Meßwerten könnte man
  dagegen die entsprechende integrale Form von \eqnref{redundancy2} verwenden. Hierbei
  gehen die Summen in Integrale und die Wahrscheinlichkeiten in die entsprechenden Dichten
  über. Da wir jedoch nur endlich viele Meßwerte haben, ist diese Vorgehensweise hier
  nicht anwendbar.}  Als Partitionen wählen wir Intervalle $I^1_i=[\xi_i,\xi_{i+1}[$
bzw.\ Produkte von Intervallen $I^2_{i,j}=[\xi_i,\xi_{i+1}[\times[\xi_j,\xi_{j+1}[$. Die
Wahrscheinlichkeit $\Prob_X(i)$ ist dann gleich dem Anteil der Punkte $N_i$, der in das
Intervall $I^1_i$ fällt, d.h.\ $\Prob_X(i)=N_i/N$. Für die Verbundwahrscheinlichkeiten
gilt entsprechend $\Prob_{x,\delay}(i,j)=N_{ij}/N$.

Offen ist noch wie die Grenzen der Intervalle $I^1_i$ gewählt werden sollen. Es
existieren dazu verschiedene Ansätze.
\begin{itemize}
\item Der erste benutzt einfach äquidistante Grenzen $x_i$. Die Länge der Intervalle
  wird auf einen bestimmten Teil der Varianz\footnote{Die Spannweite der Meßwerte wäre
    hier ein schlechtes Maß, da bei experimentellen Systemen öfters \slang(Ausreißer)
    vorkommen, die die Spannweite stark verbreitern, sonst aber nicht viel beitragen.} der
  Meßwerte festgelegt. In den hier benutzten Beispielen wird die Intervallänge auf
  $1/50$ der Varianz der Daten eingestellt.
\item Eine weitere Methode arbeitet mit Intervallen $I^1_i$, die so festgelegt werden,
  daß die Wahrscheinlichkeiten $\Prob_X(i)$ für alle Intervalle nahezu gleich sind.
  Hierzu wird die Zeitreihe $x_i$ der Größe nach sortiert. Die sortierte Reihe werde mit
  $x_{(i)}$ bezeichnet.  Offensichtlich haben Intervalle $[x_{(i)},x_{(i+k)}[$ die
  Wahrscheinlichkeit $\Prob_X(x_{(i)}\leq x<x_{(i+k)})=k/N$. Da die Wahrscheinlichkeiten
  für alle Intervalle gleich sein sollen, legen wir für die Intervallgrenzen
  $\xi_i=x_{(\lfloor\frac{N}{n}(i-1)\rfloor+1)}$ fest, wobei $n$ die Anzahl der Intervalle
  ist \footnote{$\lfloor x \rfloor$ ist die größte ganze Zahl kleiner als $x$.}.
\item Ein von \autor(Fraser) und \autor(Swinney) vorgeschlagenes Verfahren bestimmt die
  Intervalle so, daß die Verbundwahrscheinlichkeiten $\Prob_{X_\delay}(i,j)$ für alle
  $(i,j)$ annähernd gleich werden \cite{Fraser-swinney}. Gestartet wird mit einer
  Partitionierung, die aus einem einzigen Element
  $[\xi^0_1,\xi^0_2[\times[\xi^0_1,\xi^0_2[$ besteht. Dieses Element wird nun so in vier
  rechteckige Elemente zerlegt, daß in jedem Element gleich viele Paare $(x_i,x_j)$ zu
  liegen kommen. Mit den durch diese Zerlegung entstandenen Elementen $\{
  [\xi^1_k,\xi^1_{k+1} [ \times [\xi^1_l, \xi^1_{l+1} [ \, \vert 1 \leq k , l \leq 2\}$
  wird nun in derselben Weise weiter verfahren. Nach $m$ Schritten erhält man so eine
  Partitionierung des Phasenraumes mit $4^m$ gleich wahrscheinlichen Elementen
  $\{[\xi^m_k,\xi^m_{k+1}[\times[\xi^m_l,\xi^m_{l+1}[\,\vert 1\leq k,l \leq 2^m\}$.
  Das Verfahren ist jedoch recht aufwendig und bringt meiner Einschätzung nach keine
  wesentlich besseren Ergebnisse.
\item \autor(Pawelzik) entwickelte ein Verfahren, um die Redundanz aus
  verallgemeinerten Korrelationsintegralen zu bestimmen \cite{Pawelzik91}.
  Es gilt $R(X,\delay,\eps)=2C_1(1,0,\eps)-C_1(2,\delay,\eps)$, wobei $C_1(d,\delay,\eps)$
  das verallgemeinerte Korrelationsintegral\footnote{Die verallgemeinerten
    Korrelationsintegrale $C_q(\eps)$ dienen (normalerweise) der Bestimmung der verallgemeinerten Dimensionen
    $D_q$, wobei $D_q = \lim_{\eps\to0}\log C_q(\eps)/\log\eps$ gilt. } zur Einbettungsdimension $d$ und 
  Verzögerungszeit $\delay$ ist \cite{Pawelzik-schuster}. $\eps$ entspricht der
  Länge der Intervalle $I_i^1$ bei äquidistanten Grenzen. 
\end{itemize}


Die Ergebnisse für die Messung der Verzögerungszeit $\delay_R$ über das erste lokale
Minimum der Redundanz zeigen \psref{redresulta} (oben) und \psref{redresultb}
(oben). Während sich die Qualität der Rekonstruktion beim
Rössler-Attraktor nicht wesentlich verbessert hat (siehe \psref{redresulta} unten), ist
das Resultat beim Lorenz-Attraktor (siehe \psref{redresultb} unten)
deutlich besser als bei der vorherigen Rekonstruktion mit $\delay_\ac$ (siehe \psref{acfigsb} unten). Die Verzögerungszeiten sind
bei beiden Systemen kleiner geworden: beim Rössler-System von $1.29$ auf $1.16$, beim
Lorenz-System sogar von $2.50$ auf $0.162$.  

Die über die
Redundanzanalyse bestimmten Verzögerungszeiten $\delay_R$ sind immer kleiner oder gleich den
über den Nulldurchgang der Autokorrelationsfunktion bestimmten $\delay_\ac$. Man kann
sich dies in etwa so plausibel machen, daß die bei der Verzögerungszeit $\delay_\ac$
vorliegende lineare Unabhängigkeit der Koordinaten auch immer eine allgemeine
Unabhängigkeit impliziert.

\noafterpage{
  \epsfigdouble{redundancy/roemut}{redundancy/roerec2}
  {Redundanz $R(\delay)$ (oben) und der zum ersten lokalen Minimum von $R(\delay)$
    rekonstruierte Attraktoren (unten) für das Rössler-System ($\delay_R=1.16$).
    }{redresulta}{-0.2cm} 
  \epsfigdouble{redundancy/lormut}{redundancy/lorrec2}
  {Redundanz $R(\delay)$ (oben) und der zum ersten lokalen Minimum von $R(\delay)$
    rekonstruierte Attraktoren (unten) für das Lorenz-System ($\delay_R=0.162$).
    }{redresultb}{-0.2cm} 
} 

\comment{
\noafterpage{
  \epsfigdouble{autocorr/roecmp}{autocorr/lorcmp} {Vergleich der Autokorrelationsfunktion
    $\ac(\delay)$ mit der Redundanz $R(\delay)$. Die Verzögerungszeiten sind für die
    Redundanz kleiner als die für die Autokorrelationsfunktion.  }{acfigs3}{-0.2cm} }
}
\clearpage










